{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook does the following:\n",
    "    # A. Queries our database to construct sentence level data from court commitment and sentence computation for every\n",
    "    # infraction resulting in incarceration. (dataset A)\n",
    "    # B. Queries sentence component to get Most Serious Offense from all sentence components since this variable\n",
    "    # is missing in much of dataset A and is needed as our outcome variable (dataset B)\n",
    "    # C. Puts together dataset A and B\n",
    "    # D. Carries out several steps of cleaning the data and getting recidivism flag\n",
    "    # E. Queries database for any additional features (e.g. disciplinary infractions)\n",
    "    # F. Hold outs active sentences, drops those missing recidivism flag\n",
    "    # Dropped observations missing the following (if we can't proxy for them)\n",
    "        # Sentence Start Date (~1.3%)\n",
    "        # Sentence End Date (~800 obs)\n",
    "        # Most Serious Offense (2.6% obs)\n",
    "        # Our decided category (~1% obs)\n",
    "    # F. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from sqlite3 import Error\n",
    "import pandas as pd\n",
    "import config\n",
    "import os.path\n",
    "from os import path\n",
    "\n",
    "from create_db import create_connection, create_table, clean_column_names\n",
    "from populate_db import extract_data, insert_records\n",
    "import query_db as qd\n",
    "\n",
    "import importlib\n",
    "\n",
    "import datetime\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'query_db' from '/Users/daminisharma/Dropbox/Harris MSCAPP/2019-20_Q3_Spring/Machine Learning/covid_decarceration/files/query_db.py'>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(qd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coded_offenses = pd.read_excel('https://github.com/christi-liongson/covid_decarceration/blob/construct_public_safety_data/data/Coding%20Offenses%20-%20For%20GitHub.xlsx',sheet_name=\"Coding - FINAL\")\n",
    "coded_offenses = pd.read_excel('../data/Coding Offenses - For GitHub.xlsx',sheet_name=\"Coding - FINAL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Primary offense code</th>\n",
       "      <th>Description (if needed)</th>\n",
       "      <th>Decided Category</th>\n",
       "      <th>Needed a check?</th>\n",
       "      <th>More lenient</th>\n",
       "      <th>More harsh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DRIV LICENSE REVOKED</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NO</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LARCENY</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>YES</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DWI DRIVING WHILE IMPAIRED</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>NO</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FELONY B&amp;E</td>\n",
       "      <td>Felony Breaking and Entering, as opposed to Mi...</td>\n",
       "      <td>3</td>\n",
       "      <td>YES</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WORTHLESS CHECK</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NO</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Primary offense code  \\\n",
       "0        DRIV LICENSE REVOKED   \n",
       "1                     LARCENY   \n",
       "2  DWI DRIVING WHILE IMPAIRED   \n",
       "3                  FELONY B&E   \n",
       "4             WORTHLESS CHECK   \n",
       "\n",
       "                             Description (if needed)  Decided Category  \\\n",
       "0                                                  0                 1   \n",
       "1                                                  0                 2   \n",
       "2                                                  0                 2   \n",
       "3  Felony Breaking and Entering, as opposed to Mi...                 3   \n",
       "4                                                  0                 1   \n",
       "\n",
       "  Needed a check?  More lenient  More harsh  \n",
       "0              NO             1           1  \n",
       "1             YES             1           3  \n",
       "2              NO             2           2  \n",
       "3             YES             2           4  \n",
       "4              NO             1           1  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coded_offenses.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Elapsed: 0:03:31.354339\n"
     ]
    }
   ],
   "source": [
    "# Part A: Queries our database to construct sentence level data from court commitment and sentence computation for every\n",
    "    # infraction resulting in incarceration. (dataset A)\n",
    "start = datetime.datetime.now()\n",
    "query_court_commitment = '''\n",
    "                        SELECT A.OFFENDER_NC_DOC_ID_NUMBER as ID, \n",
    "                            A.COMMITMENT_PREFIX, \n",
    "                            A.EARLIEST_SENTENCE_EFFECTIVE_DT, \n",
    "                            A.MOST_SERIOUS_OFFENSE_CODE                              \n",
    "                        FROM OFNT3BB1 A\n",
    "                        WHERE NEW_PERIOD_OF_INCARCERATION_FL = \"Y\";\n",
    "                        '''\n",
    "\n",
    "conn = create_connection(config.database_name)\n",
    "court_small = qd.query_db_notebook(conn,query_court_commitment)\n",
    "\n",
    "\n",
    "query_sentence_comp = '''\n",
    "                            SELECT INMATE_DOC_NUMBER as ID, \n",
    "                                INMATE_COMMITMENT_PREFIX as COMMITMENT_PREFIX, \n",
    "                                INMATE_COMPUTATION_STATUS_FLAG, \n",
    "                                max(ACTUAL_SENTENCE_END_DATE) as END_DATE,\n",
    "                                max(PROJECTED_RELEASE_DATE_PRD) as PROJ_END_DATE\n",
    "                            FROM INMT4BB1\n",
    "                            GROUP BY INMATE_DOC_NUMBER, INMATE_COMMITMENT_PREFIX;\n",
    "                        '''\n",
    "\n",
    "sentence_compute_small = qd.query_db_notebook(conn,query_sentence_comp)\n",
    "\n",
    "\n",
    "query_inmt_profile = '''\n",
    "                    SELECT \n",
    "                        INMATE_DOC_NUMBER as ID,\n",
    "                        INMATE_RECORD_STATUS_CODE,\n",
    "                        INMATE_ADMIN_STATUS_CODE,\n",
    "                        DATE_OF_LAST_INMATE_MOVEMENT,\n",
    "                        TYPE_OF_LAST_INMATE_MOVEMENT,\n",
    "                        CURRENT_COMMITMENT_PREFIX,\n",
    "                        INMATE_GENDER_CODE as GENDER,\n",
    "                        INMATE_RACE_CODE as RACE,\n",
    "                        INMATE_BIRTH_DATE as BIRTH_DATE,\n",
    "                        INMATE_ETHNIC_AFFILIATION as ETHNICITY,\n",
    "                        INMATE_CONTROL_STATUS_CODE as CONTROL_STATUS,\n",
    "                        INMATE_SPECIAL_CHARACTERISTICS as SPECIAL_CHARS,\n",
    "                        TOTAL_DISCIPLINE_INFRACTIONS,\n",
    "                        LATEST_DISCIPLINE_INFRACTION,\n",
    "                        LAST_DISCIPLINE_INFRACTION_DT\n",
    "                    FROM INMT4AA1;\n",
    "                    '''\n",
    "\n",
    "query_inmt_profile = '''\n",
    "                    SELECT \n",
    "                        INMATE_DOC_NUMBER as ID,\n",
    "                        INMATE_RECORD_STATUS_CODE,\n",
    "                        INMATE_ADMIN_STATUS_CODE,\n",
    "                        DATE_OF_LAST_INMATE_MOVEMENT,\n",
    "                        TYPE_OF_LAST_INMATE_MOVEMENT,\n",
    "                        CURRENT_COMMITMENT_PREFIX,\n",
    "                        INMATE_CONTROL_STATUS_CODE as CONTROL_STATUS\n",
    "                    FROM INMT4AA1;\n",
    "                    '''\n",
    "\n",
    "inmt_profile = qd.query_db_notebook(conn,query_inmt_profile)\n",
    "\n",
    "query_offender_profile = '''\n",
    "                        SELECT \n",
    "                        OFFENDER_NC_DOC_ID_NUMBER as ID,\n",
    "                        OFFENDER_GENDER_CODE as GENDER,\n",
    "                        OFFENDER_RACE_CODE as RACE,\n",
    "                        OFFENDER_BIRTH_DATE as BIRTH_DATE,\n",
    "                        STATE_WHERE_OFFENDER_BORN as STATE_BORN,\n",
    "                        OFFENDER_ETHNIC_CODE as ETHNICITY,\n",
    "                        OFFENDER_CITIZENSHIP_CODE as CITIZENSHIP                        \n",
    "                    FROM OFNT3AA1;\n",
    "                            \n",
    "                        '''\n",
    "\n",
    "offender_profile = qd.query_db_notebook(conn,query_offender_profile)\n",
    "\n",
    "conn.close\n",
    "\n",
    "data = court_small.merge(sentence_compute_small, on=['ID','COMMITMENT_PREFIX'], how='outer')\n",
    "data = data.merge(inmt_profile, on=['ID'], how = 'left')\n",
    "data = data.merge(offender_profile, on=['ID'], how = 'left')\n",
    "#data = data.merge(disc_infraction, on=['ID'], how='left')\n",
    "\n",
    "\n",
    "stop = datetime.datetime.now()\n",
    "print(\"Time Elapsed:\", stop - start) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.to_csv('datasetA_court_sentcomp.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(903181, 19)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Elapsed: 0:06:31.817429\n"
     ]
    }
   ],
   "source": [
    "# Part B: Queries sentence component to get Most Serious Offense from all sentence components since this variable\n",
    "    # is missing in much of dataset A and is needed as our outcome variable (dataset B)\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "query_sentence_component = '''\n",
    "                            SELECT OFFENDER_NC_DOC_ID_NUMBER as ID, \n",
    "                                        COMMITMENT_PREFIX, \n",
    "                                        SENTENCE_COMPONENT_NUMBER,\n",
    "                                        PRIMARY_OFFENSE_CODE,\n",
    "                                        PRIMARY_FELONYMISDEMEANOR_CD,\n",
    "                                        SENTENCING_PENALTY_CLASS_CODE,\n",
    "                                        PRIOR_RECORD_LEVEL_CODE,\n",
    "                                        MINIMUM_SENTENCE_LENGTH,\n",
    "                                        MAXIMUM_SENTENCE_LENGTH,\n",
    "                                        SENTENCE_TYPE_CODE,\n",
    "                                        COUNTY_OF_CONVICTION_CODE\n",
    "                            FROM OFNT3CE1\n",
    "                            WHERE SENTENCE_TYPE_CODE LIKE '%PRISONS%';\n",
    "                            '''\n",
    "\n",
    "conn = create_connection(config.database_name)\n",
    "sent_comp_small = qd.query_db_notebook(conn,query_sentence_component)\n",
    "\n",
    "stop = datetime.datetime.now()\n",
    "print(\"Time Elapsed:\", stop - start) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sent_comp_small.to_csv('datasetB_sentcomponent_only_incarcerated.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "891122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daminisharma/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:66: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(823722, 5)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Part B: Queries sentence component to get Most Serious Offense from all sentence components since this variable\n",
    "    # is missing in much of dataset A and is needed as our outcome variable (dataset B)\n",
    "# Check how many unique ID and COMMITMENT_PREFIX combinations there are\n",
    "dataset_B = sent_comp_small.copy()\n",
    "grouped = dataset_B.groupby(['ID', 'COMMITMENT_PREFIX'])\n",
    "total_combinations = grouped.ngroups\n",
    "print(total_combinations)\n",
    "\n",
    "# Find the ID / COMMITMENT_PREFIX combinations that have the maximum MINIMUM_SENTENCE_LENGTH\n",
    "# We will use these combinations to filter dataset_B for PRIMARY_OFFENSE_CODE\n",
    "# Note: These might not be unique\n",
    "\n",
    "min_sentence = pd.DataFrame(dataset_B.groupby(['ID', 'COMMITMENT_PREFIX'])['MINIMUM_SENTENCE_LENGTH'].max().reset_index(name='max_min'))\n",
    "min_sentence.head(10)\n",
    "\n",
    "# Check to make sure we're not accidentally dropping any rows\n",
    "min_sentence.groupby(['ID', 'COMMITMENT_PREFIX']).ngroups\n",
    "\n",
    "# Filter dataset_B to only these rows\n",
    "filter_tuples = [tuple(x) for x in min_sentence.to_numpy()]\n",
    "\n",
    "filtered_B = dataset_B[dataset_B[['ID', 'COMMITMENT_PREFIX', 'MINIMUM_SENTENCE_LENGTH']].apply(tuple, axis=1).isin(filter_tuples)]\n",
    "filtered_B.head(10)\n",
    "\n",
    "count_nunique_offenses = pd.DataFrame(filtered_B.groupby(['ID', 'COMMITMENT_PREFIX'])['PRIMARY_OFFENSE_CODE'].nunique().reset_index(name='count'))\n",
    "count_nunique_offenses['count'].describe()\n",
    "\n",
    "\n",
    "# Pull out the ID / COMMITMENT_PREFIX combinations that are unique on max(MINIMUM_SENTENCE_LENGTH)\n",
    "unique_min_filter = [tuple(x) for x in count_nunique_offenses[count_nunique_offenses['count'] == 1][['ID', 'COMMITMENT_PREFIX']].to_numpy()]\n",
    "nonunique_min_filter = [tuple(x) for x in count_nunique_offenses[count_nunique_offenses['count'] != 1][['ID', 'COMMITMENT_PREFIX']].to_numpy()]\n",
    "\n",
    "cols_to_keep = ['ID', 'COMMITMENT_PREFIX','PRIMARY_OFFENSE_CODE','MINIMUM_SENTENCE_LENGTH', 'MAXIMUM_SENTENCE_LENGTH']\n",
    "\n",
    "filtered_B_min_unique = filtered_B[filtered_B[['ID','COMMITMENT_PREFIX']].apply(tuple, axis=1).isin(unique_min_filter)][cols_to_keep]\n",
    "filtered_B_min_unique.head()\n",
    "\n",
    "# Drop duplicate rows from filtered_B_min_unique (we know that they all have the same PRIMARY_OFFENSE_CODE)\n",
    "# Note: This method keeps the first observation, but again, this shouldn't matter\n",
    "filtered_B_min_unique.drop_duplicates(subset=['ID','COMMITMENT_PREFIX','PRIMARY_OFFENSE_CODE'],inplace=True)\n",
    "filtered_B_min_unique.head()\n",
    "\n",
    "filtered_B_min_nonunique = filtered_B[filtered_B[['ID','COMMITMENT_PREFIX']].apply(tuple, axis=1).isin(nonunique_min_filter)][cols_to_keep]\n",
    "filtered_B_min_nonunique.head()\n",
    "\n",
    "find_max_max = pd.DataFrame(filtered_B_min_nonunique.groupby(['ID', 'COMMITMENT_PREFIX'])['MAXIMUM_SENTENCE_LENGTH'].max().reset_index(name='max_max'))\n",
    "find_max_max.head()\n",
    "\n",
    "by_max_tuples = [tuple(x) for x in find_max_max.to_numpy()]\n",
    "filtered_B_max = filtered_B_min_nonunique[filtered_B_min_nonunique[['ID', 'COMMITMENT_PREFIX', 'MAXIMUM_SENTENCE_LENGTH']].apply(tuple, axis=1).isin(by_max_tuples)]\n",
    "filtered_B_max.head()\n",
    "\n",
    "count_offenses_by_max = pd.DataFrame(filtered_B_max.groupby(['ID', 'COMMITMENT_PREFIX'])['PRIMARY_OFFENSE_CODE'].nunique().reset_index(name='count'))\n",
    "count_offenses_by_max.head()\n",
    "\n",
    "# Pull out the ID and COMMITMENT_PREFIX tuples in FILTERED_B_MT1 where there is a unique PRIMARY_OFFENSE_CODE\n",
    "# after looking at the maximum of MAXIMUM_SENTENCE_LENGTH\n",
    "unique_max = count_offenses_by_max[count_offenses_by_max['count'] == 1][['ID', 'COMMITMENT_PREFIX']]\n",
    "unique_max_filter = [tuple(x) for x in unique_max.to_numpy()]\n",
    "\n",
    "filtered_B_max_unique = filtered_B_max[filtered_B_max[['ID', 'COMMITMENT_PREFIX']].apply(tuple, axis=1).isin(unique_max_filter)]\n",
    "filtered_B_max_unique.head()\n",
    "\n",
    "# Drop duplicate rows from filtered_B_max_unique (we know that they all have the same PRIMARY_OFFENSE_CODE)\n",
    "# Note: This method keeps the first observation, but again, this shouldn't matter\n",
    "filtered_B_max_unique.drop_duplicates(subset=['ID','COMMITMENT_PREFIX','PRIMARY_OFFENSE_CODE'],inplace=True)\n",
    "filtered_B_max_unique.head()\n",
    "\n",
    "concat_1_2 = filtered_B_min_unique.append(filtered_B_max_unique)\n",
    "concat_1_2.shape\n",
    "\n",
    "# Final merged version of datasets A and B\n",
    "dataset_with_most_serious = concat_1_2\n",
    "dataset_with_most_serious.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset B # observations: 823722\n"
     ]
    }
   ],
   "source": [
    "# Part C: Puts together dataset A and B\n",
    "datasetB_primary_offense = dataset_with_most_serious.loc[:,['ID','COMMITMENT_PREFIX','PRIMARY_OFFENSE_CODE']]\n",
    "\n",
    "print(\"Dataset B # observations:\",datasetB_primary_offense.shape[0])\n",
    "\n",
    "# merging on datasetA (court commitment + sentence computation) with datasetB (\"self constructed\" primary offenses from\n",
    "# sentence component)\n",
    "data_A_B = data.merge(datasetB_primary_offense, on = ['ID','COMMITMENT_PREFIX'], how='left') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% missing most serious offense: 0.03883602640442347\n",
      "Total number of observations in dataset A + B:  903182\n",
      "Cleaning dates and dropping missing\n",
      "Total number of observations in dataset A + B:  888121\n",
      "% still missing most serious offense: 0.02623966779301469\n",
      "Drop observations missing most serious offense code\n",
      "Total number of observations in dataset A + B:  864817\n",
      "Querying database to get nextPrefix, nextOffense\n",
      "Time Elapsed: 0:02:37.471155\n"
     ]
    }
   ],
   "source": [
    "# Part D: Carries out several steps of cleaning the data and getting recidivism flag\n",
    "\n",
    "# Replace Most Serious Offense with our constructed Primary Offense Code if missing\n",
    "data_A_B['MOST_SERIOUS_OFFENSE_CODE'].mask(data_A_B['MOST_SERIOUS_OFFENSE_CODE'].isnull(), data_A_B['PRIMARY_OFFENSE_CODE'], inplace=True)\n",
    "\n",
    "print(\"% missing most serious offense:\",data_A_B['MOST_SERIOUS_OFFENSE_CODE'].isnull().sum() / data_A_B.shape[0])\n",
    "print(\"Total number of observations in dataset A + B: \", data_A_B.shape[0])\n",
    "\n",
    "# Step 1\n",
    "# https://kanoki.org/2019/07/17/pandas-how-to-replace-values-based-on-conditions/\n",
    "print(\"Cleaning dates and dropping missing\")\n",
    "data_A_B['END_DATE'].mask(data_A_B['END_DATE'] == '0001-01-01', data_A_B['PROJ_END_DATE'], inplace=True)\n",
    "data_A_B = data_A_B[data_A_B['END_DATE']!='0001-01-01']\n",
    "data_A_B = data_A_B[data_A_B['EARLIEST_SENTENCE_EFFECTIVE_DT']!='0001-01-01']\n",
    "data_A_B = data_A_B[data_A_B['END_DATE'].notna()]\n",
    "data_A_B = data_A_B[data_A_B['EARLIEST_SENTENCE_EFFECTIVE_DT'].notna()]\n",
    "\n",
    "print(\"Total number of observations in dataset A + B: \", data_A_B.shape[0])\n",
    "print(\"% still missing most serious offense:\",data_A_B['MOST_SERIOUS_OFFENSE_CODE'].isnull().sum() / data_A_B.shape[0])\n",
    "\n",
    "# Step 1.5 drop observations missing most serious offense code\n",
    "print(\"Drop observations missing most serious offense code\")\n",
    "data_A_B = data_A_B[data_A_B['MOST_SERIOUS_OFFENSE_CODE'].notna()]\n",
    "print(\"Total number of observations in dataset A + B: \", data_A_B.shape[0])\n",
    "\n",
    "# Step 2\n",
    "# write data to sqlite in memory so can query it to get next record\n",
    "print(\"Querying database to get nextPrefix, nextOffense\")\n",
    "conn = sqlite3.connect(':memory:')\n",
    "data_A_B.to_sql('data', conn, index=False)\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "# https://stackoverflow.com/questions/37360901/sql-self-join-compare-current-record-with-the-record-of-the-previous-date\n",
    "query_datasetAB = '''\n",
    "                        SELECT *, \n",
    "                        LEAD(COMMITMENT_PREFIX,1,0) OVER (\n",
    "                                                    PARTITION BY ID\n",
    "                                                    ORDER BY COMMITMENT_PREFIX\n",
    "                                                    ) NextPrefix,\n",
    "                        LEAD(EARLIEST_SENTENCE_EFFECTIVE_DT,1,0) OVER (\n",
    "                                                    PARTITION BY ID\n",
    "                                                    ORDER BY COMMITMENT_PREFIX\n",
    "                                                    ) NextStart,\n",
    "                        LEAD(MOST_SERIOUS_OFFENSE_CODE,1,0) OVER (\n",
    "                                                    PARTITION BY ID\n",
    "                                                    ORDER BY COMMITMENT_PREFIX\n",
    "                                                    ) NextOffense                                                    \n",
    "                                                    \n",
    "                        FROM data ;\n",
    "\n",
    "                        '''\n",
    "\n",
    "\n",
    "dataset_flag = qd.query_db_notebook(conn,query_datasetAB)\n",
    "conn.close\n",
    "stop = datetime.datetime.now()\n",
    "print(\"Time Elapsed:\", stop - start) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions that fix dates\n",
    "# specifically, some dates are top coded as 9999- usually for a life sentence\n",
    "# this exceeds pandas' max date, so they first need to be re-top-coded, then turned into the date format\n",
    "# date == 0 happens when an individual does NOT have a \"next date\" - these should be turned to Na\n",
    "def fix_dates(data,date_var):\n",
    "    data['new_col'] = data[date_var].astype(str).str[0:4].astype(int)\n",
    "    data.loc[data['new_col']>2261, date_var] = '2261-01-02'\n",
    "    data[date_var] = data[date_var].replace(0,np.nan)\n",
    "    data.loc[data[date_var]==\"0\", date_var] = None\n",
    "    data[date_var] = pd.to_datetime(data[date_var],format='%Y-%m-%d',errors='coerce')\n",
    "    #df[date_var] = pd.to_datetime(df[date_var].str.split(n=1).str[0],format='%Y-%m-%d')\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_recidivism_label(data,num_years=1):\n",
    "    data['Time_Diff'] = pd.DatetimeIndex(data['NextStart']).year - pd.DatetimeIndex(data['END_DATE']).year\n",
    "    data['Recidivate'] = np.nan\n",
    "    # if NextPrefix != 0:\n",
    "    data.loc[(data['NextPrefix']!=0) & (data['Time_Diff']<= num_years) & (data['Time_Diff']>=0), 'Recidivate'] = 1\n",
    "    data.loc[(data['NextPrefix']!=0) & (data['Time_Diff']> num_years), 'Recidivate'] = 0\n",
    "    # dealing with small amount of negative Time_diff - data errors or concurrent sentences\n",
    "    data.loc[(data['NextPrefix']!=0) & (data['Time_Diff']< 0), 'Recidivate'] = 0\n",
    "    \n",
    "    \n",
    "    # if nextprefix = 0, inmate is inactive, and they did not die in prison \n",
    "    # (e.g. serving life sentence or  other wise) then \n",
    "    # recidivism = 0\n",
    "    data.loc[(data['NextPrefix']==0) & (data['INMATE_ADMIN_STATUS_CODE']=='INACTIVE') & (data['TYPE_OF_LAST_INMATE_MOVEMENT']!='DEATH'), 'Recidivate'] = 0\n",
    "    \n",
    "    # if nextprefix = 0, inmate status code is not active or inactive(could be missing) and \n",
    "    # end date is not 2261-01-02 (life sentence), they were likely released from prison\n",
    "    # recidivism = 0\n",
    "    data.loc[(data['NextPrefix']==0) & (data['INMATE_ADMIN_STATUS_CODE']!='ACTIVE') & (data['INMATE_ADMIN_STATUS_CODE']!='INACTIVE') & (data['END_DATE']!='2261-01-02'), 'Recidivate'] = 0\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fix Dates\n",
      "Get recidivism flag\n"
     ]
    }
   ],
   "source": [
    "# Part D continued\n",
    "# Step 3. \n",
    "# call fix dates function to fix relevant dates\n",
    "print(\"Fix Dates\")\n",
    "dataset_flag = fix_dates(dataset_flag,'EARLIEST_SENTENCE_EFFECTIVE_DT')\n",
    "dataset_flag = fix_dates(dataset_flag,'END_DATE')\n",
    "dataset_flag = fix_dates(dataset_flag,'NextStart')\n",
    "\n",
    "# Step 4\n",
    "# get recidivism flag - see decision rules and function above \n",
    "print(\"Get recidivism flag\")\n",
    "dataset_flag = get_recidivism_label(dataset_flag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function Connection.close>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Part E - querying additional features\n",
    "conn = create_connection(config.database_name)\n",
    "dataset_flag.to_sql('dataset_AB', conn,if_exists='replace', index=False)\n",
    "\n",
    "query = '''\n",
    "        SELECT INMATE_DOC_NUMBER as ID,\n",
    "                DISCIPLINARY_INFRACTION_DATE,\n",
    "                COMMITMENT_PREFIX,\n",
    "                EARLIEST_SENTENCE_EFFECTIVE_DT,\n",
    "                END_DATE,\n",
    "                COUNT(DISCIPLINARY_INFRACTION_DATE) as INFRACTION_PER_SENT\n",
    "        FROM INMT9CF1 A\n",
    "        INNER JOIN dataset_AB B\n",
    "        WHERE A.INMATE_DOC_NUMBER = B.ID\n",
    "        AND A.DISCIPLINARY_INFRACTION_DATE >= B.EARLIEST_SENTENCE_EFFECTIVE_DT\n",
    "        AND A.DISCIPLINARY_INFRACTION_DATE <= B.END_DATE\n",
    "        GROUP BY INMATE_DOC_NUMBER, COMMITMENT_PREFIX\n",
    "        ;\n",
    "        \n",
    "        '''\n",
    "\n",
    "disc_infraction = qd.query_db_notebook(conn,query)\n",
    "conn.close\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disc Infractions (362962, 3)\n"
     ]
    }
   ],
   "source": [
    "# Divide infractions by # of sentences if there are dups on ID / DISCIPLINARY_INFRACTION_DATE\n",
    "    # might indicate concurrent sentences\n",
    "count_dups = disc_infraction.groupby(['ID','DISCIPLINARY_INFRACTION_DATE'])[\"ID\"].count().reset_index(name=\"count\")\n",
    "disc_infraction = disc_infraction.merge(count_dups, how = 'left')\n",
    "disc_infraction['INFRACTION_PER_SENT'] = round(disc_infraction['INFRACTION_PER_SENT']/disc_infraction['count'])\n",
    "\n",
    "disc_infraction = disc_infraction.loc[:,['ID','COMMITMENT_PREFIX','INFRACTION_PER_SENT']]\n",
    "print(\"Disc Infractions\",disc_infraction.shape)\n",
    "\n",
    "# Merge on disciplinary infractions, replace missing to 0\n",
    "dataset_flag = dataset_flag.merge(disc_infraction, how='left', on=['ID','COMMITMENT_PREFIX'])\n",
    "dataset_flag.loc[dataset_flag['INFRACTION_PER_SENT'].isnull(),'INFRACTION_PER_SENT'] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hold out active sentences\n",
      "Size of active sentences dataset:  32801\n",
      "Drop observations with no recidivism flag (this will also drop active sentences, but we've already separated those)\n",
      "Additional observations dropped are mostly of those who died in prison and therefore wont have a recidivate flag\n",
      "Size of remaining dataset:  827731\n",
      "Merging on our coded categories\n",
      "% missing decided category 0.007752518632260964\n"
     ]
    }
   ],
   "source": [
    "# Part F\n",
    "# Step 5\n",
    "# Hold out active senteces\n",
    "print(\"Hold out active sentences\")\n",
    "active_sentences = dataset_flag[(dataset_flag['INMATE_ADMIN_STATUS_CODE']=='ACTIVE') & (dataset_flag['NextPrefix']==0)]\n",
    "print(\"Size of active sentences dataset: \",active_sentences.shape[0])\n",
    "\n",
    "# Step 6\n",
    "# drop observations with no recidivism flag (this will also drop active sentences, but we've already separated those)\n",
    "print(\"Drop observations with no recidivism flag (this will also drop active sentences, but we've already separated those)\")\n",
    "print(\"Additional observations dropped are mostly of those who died in prison and therefore wont have a recidivate flag\")\n",
    "dataset_flag = dataset_flag[(dataset_flag['Recidivate'].notnull())]\n",
    "print(\"Size of remaining dataset: \",dataset_flag.shape[0])\n",
    "\n",
    "# Step 7\n",
    "# Bring in coded offenses - sanity check\n",
    "\n",
    "# this merges our coded offenses onto \"most serious offense\" to check how much coverage\n",
    "# our variable is giving us. however, this not what we ultimately want - in the end, we want\n",
    "# our codes to be merged onto \"nextOffense\" - i.e., the offense code for the next offense \n",
    "# someone committed that resulted in re-incarceration\n",
    "# NextOffense can be missing for 2 reasons: because most serious offense is missing, or because\n",
    "# the individual did not recidivate. after merging our codes onto \"NextOffense\", we can replace\n",
    "# \"Decided Category\" with 0 if recidivism = 0, and leave it as NA otherwise\n",
    "#dataset_with_offenses_test = dataset_flag.merge(coded_offenses, how='left', left_on='MOST_SERIOUS_OFFENSE_CODE', right_on='Primary offense code')\n",
    "\n",
    "# Step 8 and 9\n",
    "# Now, merge on coded offenses onto NextOffense, turn Decided Category, More Lenient, and more harsh = 0 if recidivism = 0\n",
    "print(\"Merging on our coded categories\")\n",
    "dataset_with_offenses = dataset_flag.merge(coded_offenses, how='left', left_on='NextOffense', right_on='Primary offense code')\n",
    "dataset_with_offenses.loc[dataset_with_offenses['Recidivate']==0,'Decided Category'] = 0\n",
    "dataset_with_offenses.loc[dataset_with_offenses['Recidivate']==0,'More lenient'] = 0\n",
    "dataset_with_offenses.loc[dataset_with_offenses['Recidivate']==0,'More harsh'] = 0\n",
    "\n",
    "print(\"% missing decided category\",dataset_with_offenses['Decided Category'].isnull().sum()/dataset_with_offenses.shape[0])\n",
    "\n",
    "# Drop those missing decided category\n",
    "dataset_with_offenses = dataset_with_offenses[(dataset_with_offenses['Decided Category'].notnull())]\n",
    "print(\"Final dataset size: \" , dataset_with_offenses.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_with_offenses.to_csv('../data/dataset_main.csv', index=False)\n",
    "active_sentences.to_csv('../data/active_sentences.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As detailed above, here's where we stand with \"most serious offense code\"\n",
    "    - 33% of dataset A is missing most_serious_offense\n",
    "    - using sentence component, we created primary offense code for about 92% of the sentence component data (using minimum and maximum length) \n",
    "    - this variable (call it Offense_Constructed) has a 93% match rate with MosT Serious Offense in dataset A (where its available)\n",
    "    - we're going to use Most Serious Offense where available (66% of the time), replace with Offense_Constructed where Most Serious Offense is unavailable and Offense_Constructed is available (32% of data). \n",
    "    - This will mean we are still missing Most Serious Offense for 4% of observations. Not all of these will be relevant to our outcome variable (only relevant when someone recidivates) but a) we want to use most serious offense as a predictor so missingness is relevant and b) how many of these are relevant for recidivating might keep changing depending on our # of years for recidivating. After we do all other changes to this dataset (e.g. dropping for weird dates) will check again how many are missing most serious offense. will drop those at that point (2.6% obs)\n",
    "    - Finally, ~1% of the remaining data is missing our outcome variable once it is merged on because we only coded\n",
    "       up offenses that took up 95% of all offenses. We drop these as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part C broken down in more detail\n",
    "    # 1. Deal with date issues (takes us from 903,000 obs to 888,120). For more details, see below:\n",
    "        # a. replace end date with projected end date where END_DATE = 0001-01-01 (placeholder for missing)\n",
    "        # b. drop observations still missing end_date (should be only about ~350 observations)\n",
    "        # c. drop observations missing EARLIEST_SENTENCE_EFFECTIVE_DT (about 12k observations)\n",
    "        # b and c are dropping those where the sentence is either only in court commitment or only in sentence comp\n",
    "    # 2. Query the remaining dataset to get the the next commitment prefix, next sentence date, and most serious\n",
    "        # offense code for the next observation - where all of these exist. for a sentence that does not result in \n",
    "        # recidivism, nextprefix, nextstart, nextoffense will be 0\n",
    "    # 3. Clean up dates - turn them into date format, after recoding the top coded 9999 dates (for life sentences)\n",
    "    # 4. Get recidivism flags. See decision rule below\n",
    "    # 5. Hold out active sentences (~approx 32,000 obs)\n",
    "    # 6. Drop observation with no recidivism flag (Takes us from 888,120 to 850970, i.e. dropping\n",
    "        # 38,000 observations. 32k of those are active sentences, 6k are \"out of universe\" i.e.\n",
    "        # sentences that are expired but the individual was never released (mostly death in prison)\n",
    "    # 7. Sanity check - Merge on our coded offenses to most serious offenses and see how well we cover the offenses\n",
    "        # Approx 5% of observations that have Most Serious Offense do NOT have \"Decided Category\" (our variable)\n",
    "        # This makes sense because we only coded up offenses that made up 95% of the offenses\n",
    "    # 8. Merge on our coded offenses to \"NextOffense\" - the relevant variable now is \"Decided category\".\n",
    "    # 9. Replace Decided Category to 0 if recidivism = 0 ; leave it as NA otherwise\n",
    "        # After holding out active sentences and dropping \"out of universe observations\", we have ~850k observations\n",
    "        # of these, we are missing a \"Decided Category\" flag (as defined by our coded offenses) for 7% of the data\n",
    "        # this is a lot better than missing it for 33% of the data (since we're missing \"most serious offense\" for \n",
    "        # 33% of the data) but its still not great - hopefully once we bring in most serious offense from sentence\n",
    "        # component, we can reduce 7% down to something more negligible\n",
    "    # 10. Understand the missingness of our possible features\n",
    "    \n",
    "# We now have two datasets that are ready for pre processing and feature engineering:\n",
    "    # dataset_with_offenses = datasetA \n",
    "    # active_sentences = data on which we will apply our predictions\n",
    "        # Next steps (I think): develop a list of features and functions that can clean up those features, which can \n",
    "        # be applied to both of the datasets above\n",
    "        # Additionally - do we want to write both of these to csv that we push to github?\n",
    "    \n",
    "# More details on Dates\n",
    "    # In addition to the dates that are null (see above) because some data exists in court commitment\n",
    "    # that doesnt exist in sentence computation (and vice versa) we also have start and end\n",
    "    # dates that are 0001-01-01 - based on looking up some offenders with these dates, these\n",
    "    # are often just missing so 0001-01-01 is a placeholder for missing date\n",
    "\n",
    "    # There are about 10k observations with end_date = 0001-01-01. These don't seem random -\n",
    "    # 9k of these are for the commitment prefix BA, and on spot checking many of them look like\n",
    "    # the sentences were categorized as \"FAIR FELONS\" - related to the fair sentencing act that\n",
    "    # affects sentences from 1982 to October 1994 (before NC enacted structured sentencing which\n",
    "    # abolished parole). It also seems like many of those sentences are missing an \"actual release\n",
    "    # date\" from prison but have a release date from parole\n",
    "    # \n",
    "    # Where available, the end date will be replaced with the projected release date. on spot\n",
    "    # checking, this seems to be a reasonable proxy for when inmate was last moved\n",
    "    # There are 397 observations missing both end date and projected end date - dropping these\n",
    "    #\n",
    "    # About 12k observations have start date = 0001-01-01. On spot checking, some of these\n",
    "    # appear to be entirely missing from sentence component and from the offender's online\n",
    "    # profile - as if the sentences were removed ex-post. Since there is no way to get a start\n",
    "    # date for these, they will be dropped. Approx 1% of the data\n",
    "\n",
    "# Note on \"Active\" Flag    \n",
    "    # To get \"Active\" sentences, we should probably not trust the Inmate Commitment\n",
    "    # status flag in court commitment. This often appears active even for sentences that\n",
    "    # online show \"service status\" = \"Expired\"\n",
    "\n",
    "    # instead, we should merge on information from INmate Profile. This has \"inmate record status\"\n",
    "    # and \"inmate admin status\". After some exploration, it seems like admin status = active\n",
    "    # means one is in prison; record status = active (if admin status = inactive) is mostly for\n",
    "    # people on parole/probation.\n",
    "\n",
    "\n",
    "# Decision rule for recidivism flag:\n",
    "    # if NextPrefix != 0: if nextStart - endDate is less than XXX (make this a parameter) then recidivism = 1 else 0\n",
    "\n",
    "    # if nextprefix = 0, inmate is inactive, and they did not die in prison \n",
    "    # (e.g. serving life sentence or  other wise) then \n",
    "    # recidivism = 0\n",
    "\n",
    "    # if nextprefix = 0, inmate status code is not active or inactive (could be missing) and \n",
    "    # end date is not 2261-01-02 (life sentence), they were likely released from prison\n",
    "    # recidivism = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
