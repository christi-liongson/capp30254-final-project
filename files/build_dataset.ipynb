{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook does the following:\n",
    "    # A. Queries our database to construct sentence level data from court commitment and sentence computation for every\n",
    "    # infraction resulting in incarceration. (dataset A)\n",
    "    # B. Queries sentence component to get Most Serious Offense from all sentence components since this variable\n",
    "    # is missing in much of dataset A and is needed as our outcome variable (dataset B)\n",
    "    # C. Puts together dataset A and B\n",
    "    # D. Carries out several steps of cleaning the data and getting recidivism flag\n",
    "    # E. Queries database for any additional features (e.g. disciplinary infractions)\n",
    "    # F. Hold outs active sentences, drops those missing recidivism flag\n",
    "    # Dropped observations missing the following (if we can't proxy for them)\n",
    "        # Sentence Start Date (~1.3%)\n",
    "        # Sentence End Date (~800 obs)\n",
    "        # Most Serious Offense (2.6% obs)\n",
    "        # Our decided category (~1% obs)\n",
    "    # F. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from sqlite3 import Error\n",
    "import pandas as pd\n",
    "import config\n",
    "import os.path\n",
    "from os import path\n",
    "\n",
    "from create_db import create_connection, create_table, clean_column_names\n",
    "from populate_db import extract_data, insert_records\n",
    "import query_db as qd\n",
    "\n",
    "import importlib\n",
    "\n",
    "import datetime\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'query_db' from '/Users/daminisharma/Dropbox/Harris MSCAPP/2019-20_Q3_Spring/Machine Learning/covid_decarceration/files/query_db.py'>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(qd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coded_offenses = pd.read_excel('https://github.com/christi-liongson/covid_decarceration/blob/construct_public_safety_data/data/Coding%20Offenses%20-%20For%20GitHub.xlsx',sheet_name=\"Coding - FINAL\")\n",
    "coded_offenses = pd.read_excel('../data/Coding Offenses - For GitHub.xlsx',sheet_name=\"Coding - FINAL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Primary offense code</th>\n",
       "      <th>Description (if needed)</th>\n",
       "      <th>Decided Category</th>\n",
       "      <th>Needed a check?</th>\n",
       "      <th>More lenient</th>\n",
       "      <th>More harsh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DRIV LICENSE REVOKED</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NO</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LARCENY</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>YES</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DWI DRIVING WHILE IMPAIRED</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>NO</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FELONY B&amp;E</td>\n",
       "      <td>Felony Breaking and Entering, as opposed to Mi...</td>\n",
       "      <td>3</td>\n",
       "      <td>YES</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WORTHLESS CHECK</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NO</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Primary offense code  \\\n",
       "0        DRIV LICENSE REVOKED   \n",
       "1                     LARCENY   \n",
       "2  DWI DRIVING WHILE IMPAIRED   \n",
       "3                  FELONY B&E   \n",
       "4             WORTHLESS CHECK   \n",
       "\n",
       "                             Description (if needed)  Decided Category  \\\n",
       "0                                                  0                 1   \n",
       "1                                                  0                 2   \n",
       "2                                                  0                 2   \n",
       "3  Felony Breaking and Entering, as opposed to Mi...                 3   \n",
       "4                                                  0                 1   \n",
       "\n",
       "  Needed a check?  More lenient  More harsh  \n",
       "0              NO             1           1  \n",
       "1             YES             1           3  \n",
       "2              NO             2           2  \n",
       "3             YES             2           4  \n",
       "4              NO             1           1  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coded_offenses.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Elapsed: 0:03:31.354339\n"
     ]
    }
   ],
   "source": [
    "# Part A: Queries our database to construct sentence level data from court commitment and sentence computation for every\n",
    "    # infraction resulting in incarceration. (dataset A)\n",
    "start = datetime.datetime.now()\n",
    "query_court_commitment = '''\n",
    "                        SELECT A.OFFENDER_NC_DOC_ID_NUMBER as ID, \n",
    "                            A.COMMITMENT_PREFIX, \n",
    "                            A.EARLIEST_SENTENCE_EFFECTIVE_DT, \n",
    "                            A.MOST_SERIOUS_OFFENSE_CODE                              \n",
    "                        FROM OFNT3BB1 A\n",
    "                        WHERE NEW_PERIOD_OF_INCARCERATION_FL = \"Y\";\n",
    "                        '''\n",
    "\n",
    "conn = create_connection(config.database_name)\n",
    "court_small = qd.query_db_notebook(conn,query_court_commitment)\n",
    "\n",
    "\n",
    "query_sentence_comp = '''\n",
    "                            SELECT INMATE_DOC_NUMBER as ID, \n",
    "                                INMATE_COMMITMENT_PREFIX as COMMITMENT_PREFIX, \n",
    "                                INMATE_COMPUTATION_STATUS_FLAG, \n",
    "                                max(ACTUAL_SENTENCE_END_DATE) as END_DATE,\n",
    "                                max(PROJECTED_RELEASE_DATE_PRD) as PROJ_END_DATE\n",
    "                            FROM INMT4BB1\n",
    "                            GROUP BY INMATE_DOC_NUMBER, INMATE_COMMITMENT_PREFIX;\n",
    "                        '''\n",
    "\n",
    "sentence_compute_small = qd.query_db_notebook(conn,query_sentence_comp)\n",
    "\n",
    "\n",
    "query_inmt_profile = '''\n",
    "                    SELECT \n",
    "                        INMATE_DOC_NUMBER as ID,\n",
    "                        INMATE_RECORD_STATUS_CODE,\n",
    "                        INMATE_ADMIN_STATUS_CODE,\n",
    "                        DATE_OF_LAST_INMATE_MOVEMENT,\n",
    "                        TYPE_OF_LAST_INMATE_MOVEMENT,\n",
    "                        CURRENT_COMMITMENT_PREFIX,\n",
    "                        INMATE_GENDER_CODE as GENDER,\n",
    "                        INMATE_RACE_CODE as RACE,\n",
    "                        INMATE_BIRTH_DATE as BIRTH_DATE,\n",
    "                        INMATE_ETHNIC_AFFILIATION as ETHNICITY,\n",
    "                        INMATE_CONTROL_STATUS_CODE as CONTROL_STATUS,\n",
    "                        INMATE_SPECIAL_CHARACTERISTICS as SPECIAL_CHARS,\n",
    "                        TOTAL_DISCIPLINE_INFRACTIONS,\n",
    "                        LATEST_DISCIPLINE_INFRACTION,\n",
    "                        LAST_DISCIPLINE_INFRACTION_DT\n",
    "                    FROM INMT4AA1;\n",
    "                    '''\n",
    "\n",
    "query_inmt_profile = '''\n",
    "                    SELECT \n",
    "                        INMATE_DOC_NUMBER as ID,\n",
    "                        INMATE_RECORD_STATUS_CODE,\n",
    "                        INMATE_ADMIN_STATUS_CODE,\n",
    "                        DATE_OF_LAST_INMATE_MOVEMENT,\n",
    "                        TYPE_OF_LAST_INMATE_MOVEMENT,\n",
    "                        CURRENT_COMMITMENT_PREFIX,\n",
    "                        INMATE_CONTROL_STATUS_CODE as CONTROL_STATUS\n",
    "                    FROM INMT4AA1;\n",
    "                    '''\n",
    "\n",
    "inmt_profile = qd.query_db_notebook(conn,query_inmt_profile)\n",
    "\n",
    "query_offender_profile = '''\n",
    "                        SELECT \n",
    "                        OFFENDER_NC_DOC_ID_NUMBER as ID,\n",
    "                        OFFENDER_GENDER_CODE as GENDER,\n",
    "                        OFFENDER_RACE_CODE as RACE,\n",
    "                        OFFENDER_BIRTH_DATE as BIRTH_DATE,\n",
    "                        STATE_WHERE_OFFENDER_BORN as STATE_BORN,\n",
    "                        OFFENDER_ETHNIC_CODE as ETHNICITY,\n",
    "                        OFFENDER_CITIZENSHIP_CODE as CITIZENSHIP                        \n",
    "                    FROM OFNT3AA1;\n",
    "                            \n",
    "                        '''\n",
    "\n",
    "offender_profile = qd.query_db_notebook(conn,query_offender_profile)\n",
    "\n",
    "conn.close\n",
    "\n",
    "data = court_small.merge(sentence_compute_small, on=['ID','COMMITMENT_PREFIX'], how='outer')\n",
    "data = data.merge(inmt_profile, on=['ID'], how = 'left')\n",
    "data = data.merge(offender_profile, on=['ID'], how = 'left')\n",
    "#data = data.merge(disc_infraction, on=['ID'], how='left')\n",
    "\n",
    "\n",
    "stop = datetime.datetime.now()\n",
    "print(\"Time Elapsed:\", stop - start) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.to_csv('datasetA_court_sentcomp.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(903181, 19)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Elapsed: 0:06:31.817429\n"
     ]
    }
   ],
   "source": [
    "# Part B: Queries sentence component to get Most Serious Offense from all sentence components since this variable\n",
    "    # is missing in much of dataset A and is needed as our outcome variable (dataset B)\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "query_sentence_component = '''\n",
    "                            SELECT OFFENDER_NC_DOC_ID_NUMBER as ID, \n",
    "                                        COMMITMENT_PREFIX, \n",
    "                                        SENTENCE_COMPONENT_NUMBER,\n",
    "                                        PRIMARY_OFFENSE_CODE,\n",
    "                                        PRIMARY_FELONYMISDEMEANOR_CD,\n",
    "                                        SENTENCING_PENALTY_CLASS_CODE,\n",
    "                                        PRIOR_RECORD_LEVEL_CODE,\n",
    "                                        MINIMUM_SENTENCE_LENGTH,\n",
    "                                        MAXIMUM_SENTENCE_LENGTH,\n",
    "                                        SENTENCE_TYPE_CODE,\n",
    "                                        COUNTY_OF_CONVICTION_CODE\n",
    "                            FROM OFNT3CE1\n",
    "                            WHERE SENTENCE_TYPE_CODE LIKE '%PRISONS%';\n",
    "                            '''\n",
    "\n",
    "conn = create_connection(config.database_name)\n",
    "sent_comp_small = qd.query_db_notebook(conn,query_sentence_component)\n",
    "\n",
    "stop = datetime.datetime.now()\n",
    "print(\"Time Elapsed:\", stop - start) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sent_comp_small.to_csv('datasetB_sentcomponent_only_incarcerated.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "891122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daminisharma/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:66: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(823722, 5)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Part B: Queries sentence component to get Most Serious Offense from all sentence components since this variable\n",
    "    # is missing in much of dataset A and is needed as our outcome variable (dataset B)\n",
    "# Check how many unique ID and COMMITMENT_PREFIX combinations there are\n",
    "dataset_B = sent_comp_small.copy()\n",
    "grouped = dataset_B.groupby(['ID', 'COMMITMENT_PREFIX'])\n",
    "total_combinations = grouped.ngroups\n",
    "print(total_combinations)\n",
    "\n",
    "# Find the ID / COMMITMENT_PREFIX combinations that have the maximum MINIMUM_SENTENCE_LENGTH\n",
    "# We will use these combinations to filter dataset_B for PRIMARY_OFFENSE_CODE\n",
    "# Note: These might not be unique\n",
    "\n",
    "min_sentence = pd.DataFrame(dataset_B.groupby(['ID', 'COMMITMENT_PREFIX'])['MINIMUM_SENTENCE_LENGTH'].max().reset_index(name='max_min'))\n",
    "min_sentence.head(10)\n",
    "\n",
    "# Check to make sure we're not accidentally dropping any rows\n",
    "min_sentence.groupby(['ID', 'COMMITMENT_PREFIX']).ngroups\n",
    "\n",
    "# Filter dataset_B to only these rows\n",
    "filter_tuples = [tuple(x) for x in min_sentence.to_numpy()]\n",
    "\n",
    "filtered_B = dataset_B[dataset_B[['ID', 'COMMITMENT_PREFIX', 'MINIMUM_SENTENCE_LENGTH']].apply(tuple, axis=1).isin(filter_tuples)]\n",
    "filtered_B.head(10)\n",
    "\n",
    "count_nunique_offenses = pd.DataFrame(filtered_B.groupby(['ID', 'COMMITMENT_PREFIX'])['PRIMARY_OFFENSE_CODE'].nunique().reset_index(name='count'))\n",
    "count_nunique_offenses['count'].describe()\n",
    "\n",
    "\n",
    "# Pull out the ID / COMMITMENT_PREFIX combinations that are unique on max(MINIMUM_SENTENCE_LENGTH)\n",
    "unique_min_filter = [tuple(x) for x in count_nunique_offenses[count_nunique_offenses['count'] == 1][['ID', 'COMMITMENT_PREFIX']].to_numpy()]\n",
    "nonunique_min_filter = [tuple(x) for x in count_nunique_offenses[count_nunique_offenses['count'] != 1][['ID', 'COMMITMENT_PREFIX']].to_numpy()]\n",
    "\n",
    "cols_to_keep = ['ID', 'COMMITMENT_PREFIX','PRIMARY_OFFENSE_CODE','MINIMUM_SENTENCE_LENGTH', 'MAXIMUM_SENTENCE_LENGTH']\n",
    "\n",
    "filtered_B_min_unique = filtered_B[filtered_B[['ID','COMMITMENT_PREFIX']].apply(tuple, axis=1).isin(unique_min_filter)][cols_to_keep]\n",
    "filtered_B_min_unique.head()\n",
    "\n",
    "# Drop duplicate rows from filtered_B_min_unique (we know that they all have the same PRIMARY_OFFENSE_CODE)\n",
    "# Note: This method keeps the first observation, but again, this shouldn't matter\n",
    "filtered_B_min_unique.drop_duplicates(subset=['ID','COMMITMENT_PREFIX','PRIMARY_OFFENSE_CODE'],inplace=True)\n",
    "filtered_B_min_unique.head()\n",
    "\n",
    "filtered_B_min_nonunique = filtered_B[filtered_B[['ID','COMMITMENT_PREFIX']].apply(tuple, axis=1).isin(nonunique_min_filter)][cols_to_keep]\n",
    "filtered_B_min_nonunique.head()\n",
    "\n",
    "find_max_max = pd.DataFrame(filtered_B_min_nonunique.groupby(['ID', 'COMMITMENT_PREFIX'])['MAXIMUM_SENTENCE_LENGTH'].max().reset_index(name='max_max'))\n",
    "find_max_max.head()\n",
    "\n",
    "by_max_tuples = [tuple(x) for x in find_max_max.to_numpy()]\n",
    "filtered_B_max = filtered_B_min_nonunique[filtered_B_min_nonunique[['ID', 'COMMITMENT_PREFIX', 'MAXIMUM_SENTENCE_LENGTH']].apply(tuple, axis=1).isin(by_max_tuples)]\n",
    "filtered_B_max.head()\n",
    "\n",
    "count_offenses_by_max = pd.DataFrame(filtered_B_max.groupby(['ID', 'COMMITMENT_PREFIX'])['PRIMARY_OFFENSE_CODE'].nunique().reset_index(name='count'))\n",
    "count_offenses_by_max.head()\n",
    "\n",
    "# Pull out the ID and COMMITMENT_PREFIX tuples in FILTERED_B_MT1 where there is a unique PRIMARY_OFFENSE_CODE\n",
    "# after looking at the maximum of MAXIMUM_SENTENCE_LENGTH\n",
    "unique_max = count_offenses_by_max[count_offenses_by_max['count'] == 1][['ID', 'COMMITMENT_PREFIX']]\n",
    "unique_max_filter = [tuple(x) for x in unique_max.to_numpy()]\n",
    "\n",
    "filtered_B_max_unique = filtered_B_max[filtered_B_max[['ID', 'COMMITMENT_PREFIX']].apply(tuple, axis=1).isin(unique_max_filter)]\n",
    "filtered_B_max_unique.head()\n",
    "\n",
    "# Drop duplicate rows from filtered_B_max_unique (we know that they all have the same PRIMARY_OFFENSE_CODE)\n",
    "# Note: This method keeps the first observation, but again, this shouldn't matter\n",
    "filtered_B_max_unique.drop_duplicates(subset=['ID','COMMITMENT_PREFIX','PRIMARY_OFFENSE_CODE'],inplace=True)\n",
    "filtered_B_max_unique.head()\n",
    "\n",
    "concat_1_2 = filtered_B_min_unique.append(filtered_B_max_unique)\n",
    "concat_1_2.shape\n",
    "\n",
    "# Final merged version of datasets A and B\n",
    "dataset_with_most_serious = concat_1_2\n",
    "dataset_with_most_serious.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset B # observations: 823722\n"
     ]
    }
   ],
   "source": [
    "# Part C: Puts together dataset A and B\n",
    "datasetB_primary_offense = dataset_with_most_serious.loc[:,['ID','COMMITMENT_PREFIX','PRIMARY_OFFENSE_CODE']]\n",
    "\n",
    "print(\"Dataset B # observations:\",datasetB_primary_offense.shape[0])\n",
    "\n",
    "# merging on datasetA (court commitment + sentence computation) with datasetB (\"self constructed\" primary offenses from\n",
    "# sentence component)\n",
    "data_A_B = data.merge(datasetB_primary_offense, on = ['ID','COMMITMENT_PREFIX'], how='left') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% missing most serious offense: 0.0\n",
      "Total number of observations in dataset A + B:  864817\n",
      "Cleaning dates and dropping missing\n",
      "Total number of observations in dataset A + B:  864817\n",
      "% still missing most serious offense: 0.0\n",
      "Drop observations missing most serious offense code\n",
      "Total number of observations in dataset A + B:  864817\n",
      "Querying database to get nextPrefix, nextOffense\n",
      "Time Elapsed: 0:02:42.930559\n"
     ]
    }
   ],
   "source": [
    "# Part D: Carries out several steps of cleaning the data and getting recidivism flag\n",
    "\n",
    "# Replace Most Serious Offense with our constructed Primary Offense Code if missing\n",
    "data_A_B['MOST_SERIOUS_OFFENSE_CODE'].mask(data_A_B['MOST_SERIOUS_OFFENSE_CODE'].isnull(), data_A_B['PRIMARY_OFFENSE_CODE'], inplace=True)\n",
    "\n",
    "print(\"% missing most serious offense:\",data_A_B['MOST_SERIOUS_OFFENSE_CODE'].isnull().sum() / data_A_B.shape[0])\n",
    "print(\"Total number of observations in dataset A + B: \", data_A_B.shape[0])\n",
    "\n",
    "# Step 1\n",
    "# https://kanoki.org/2019/07/17/pandas-how-to-replace-values-based-on-conditions/\n",
    "print(\"Cleaning dates and dropping missing\")\n",
    "data_A_B['END_DATE'].mask(data_A_B['END_DATE'] == '0001-01-01', data_A_B['PROJ_END_DATE'], inplace=True)\n",
    "data_A_B = data_A_B[data_A_B['END_DATE']!='0001-01-01']\n",
    "data_A_B = data_A_B[data_A_B['EARLIEST_SENTENCE_EFFECTIVE_DT']!='0001-01-01']\n",
    "data_A_B = data_A_B[data_A_B['END_DATE'].notna()]\n",
    "data_A_B = data_A_B[data_A_B['EARLIEST_SENTENCE_EFFECTIVE_DT'].notna()]\n",
    "\n",
    "print(\"Total number of observations in dataset A + B: \", data_A_B.shape[0])\n",
    "print(\"% still missing most serious offense:\",data_A_B['MOST_SERIOUS_OFFENSE_CODE'].isnull().sum() / data_A_B.shape[0])\n",
    "\n",
    "# Step 1.5 drop observations missing most serious offense code\n",
    "print(\"Drop observations missing most serious offense code\")\n",
    "data_A_B = data_A_B[data_A_B['MOST_SERIOUS_OFFENSE_CODE'].notna()]\n",
    "print(\"Total number of observations in dataset A + B: \", data_A_B.shape[0])\n",
    "\n",
    "# Step 2\n",
    "# write data to sqlite in memory so can query it to get next record\n",
    "print(\"Querying database to get nextPrefix, nextOffense\")\n",
    "conn = sqlite3.connect(':memory:')\n",
    "data_A_B.to_sql('data', conn, index=False)\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "# https://stackoverflow.com/questions/37360901/sql-self-join-compare-current-record-with-the-record-of-the-previous-date\n",
    "query_datasetAB = '''\n",
    "                        SELECT *, \n",
    "                        LEAD(COMMITMENT_PREFIX,1,NONE) OVER (\n",
    "                                                    PARTITION BY ID\n",
    "                                                    ORDER BY COMMITMENT_PREFIX\n",
    "                                                    ) NextPrefix,\n",
    "                        LEAD(EARLIEST_SENTENCE_EFFECTIVE_DT,1,NONE) OVER (\n",
    "                                                    PARTITION BY ID\n",
    "                                                    ORDER BY COMMITMENT_PREFIX\n",
    "                                                    ) NextStart,\n",
    "                        LEAD(MOST_SERIOUS_OFFENSE_CODE,1,NONE) OVER (\n",
    "                                                    PARTITION BY ID\n",
    "                                                    ORDER BY COMMITMENT_PREFIX\n",
    "                                                    ) NextOffense                                                    \n",
    "                                                    \n",
    "                        FROM data ;\n",
    "\n",
    "                        '''\n",
    "\n",
    "\n",
    "dataset_flag = qd.query_db_notebook(conn,query_datasetAB)\n",
    "conn.close\n",
    "stop = datetime.datetime.now()\n",
    "print(\"Time Elapsed:\", stop - start) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions that fix dates\n",
    "# specifically, some dates are top coded as 9999- usually for a life sentence\n",
    "# this exceeds pandas' max date, so they first need to be re-top-coded, then turned into the date format\n",
    "# date == 0 happens when an individual does NOT have a \"next date\" - these should be turned to Na\n",
    "def fix_dates(data,date_var):\n",
    "    data['new_col'] = data[date_var].astype(str).str[0:4].astype(int)\n",
    "    data.loc[data['new_col']>2261, date_var] = '2261-01-02'\n",
    "    data[date_var] = data[date_var].replace(0,np.nan)\n",
    "    data.loc[data[date_var]==\"0\", date_var] = None\n",
    "    data[date_var] = pd.to_datetime(data[date_var],format='%Y-%m-%d',errors='coerce')\n",
    "    #df[date_var] = pd.to_datetime(df[date_var].str.split(n=1).str[0],format='%Y-%m-%d')\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_recidivism_label(data,num_years=1):\n",
    "    data['Time_Diff'] = pd.DatetimeIndex(data['NextStart']).year - pd.DatetimeIndex(data['END_DATE']).year\n",
    "    data['Recidivate'] = np.nan\n",
    "    # if NextPrefix != 0:\n",
    "    data.loc[(data['NextPrefix']!=0) & (data['Time_Diff']<= num_years) & (data['Time_Diff']>=0), 'Recidivate'] = 1\n",
    "    data.loc[(data['NextPrefix']!=0) & (data['Time_Diff']> num_years), 'Recidivate'] = 0\n",
    "    # dealing with small amount of negative Time_diff - data errors or concurrent sentences\n",
    "    data.loc[(data['NextPrefix']!=0) & (data['Time_Diff']< 0), 'Recidivate'] = 0\n",
    "    \n",
    "    \n",
    "    # if nextprefix = 0, inmate is inactive, and they did not die in prison \n",
    "    # (e.g. serving life sentence or  other wise) then \n",
    "    # recidivism = 0\n",
    "    data.loc[(data['NextPrefix']==0) & (data['INMATE_ADMIN_STATUS_CODE']=='INACTIVE') & (data['TYPE_OF_LAST_INMATE_MOVEMENT']!='DEATH'), 'Recidivate'] = 0\n",
    "    \n",
    "    # if nextprefix = 0, inmate status code is not active or inactive(could be missing) and \n",
    "    # end date is not 2261-01-02 (life sentence), they were likely released from prison\n",
    "    # recidivism = 0\n",
    "    data.loc[(data['NextPrefix']==0) & (data['INMATE_ADMIN_STATUS_CODE']!='ACTIVE') & (data['INMATE_ADMIN_STATUS_CODE']!='INACTIVE') & (data['END_DATE']!='2261-01-02'), 'Recidivate'] = 0\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fix Dates\n",
      "Get recidivism flag\n"
     ]
    }
   ],
   "source": [
    "# Part D continued\n",
    "# Step 3. \n",
    "# call fix dates function to fix relevant dates\n",
    "print(\"Fix Dates\")\n",
    "dataset_flag = fix_dates(dataset_flag,'EARLIEST_SENTENCE_EFFECTIVE_DT')\n",
    "dataset_flag = fix_dates(dataset_flag,'END_DATE')\n",
    "dataset_flag = fix_dates(dataset_flag,'NextStart')\n",
    "\n",
    "# Step 4\n",
    "# get recidivism flag - see decision rules and function above \n",
    "print(\"Get recidivism flag\")\n",
    "dataset_flag = get_recidivism_label(dataset_flag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function Connection.close>"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Part E - querying additional features\n",
    "conn = create_connection(config.database_name)\n",
    "dataset_flag.to_sql('dataset_AB', conn,if_exists='replace', index=False)\n",
    "\n",
    "query = '''\n",
    "        SELECT INMATE_DOC_NUMBER as ID,\n",
    "                DISCIPLINARY_INFRACTION_DATE,\n",
    "                COMMITMENT_PREFIX,\n",
    "                EARLIEST_SENTENCE_EFFECTIVE_DT,\n",
    "                END_DATE,\n",
    "                COUNT(DISCIPLINARY_INFRACTION_DATE) as INFRACTION_PER_SENT\n",
    "        FROM INMT9CF1 A\n",
    "        INNER JOIN dataset_AB B\n",
    "        WHERE A.INMATE_DOC_NUMBER = B.ID\n",
    "        AND A.DISCIPLINARY_INFRACTION_DATE >= B.EARLIEST_SENTENCE_EFFECTIVE_DT\n",
    "        AND A.DISCIPLINARY_INFRACTION_DATE <= B.END_DATE\n",
    "        GROUP BY INMATE_DOC_NUMBER, COMMITMENT_PREFIX\n",
    "        ;\n",
    "        \n",
    "        '''\n",
    "\n",
    "disc_infraction = qd.query_db_notebook(conn,query)\n",
    "conn.close\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disc Infractions (362962, 3)\n"
     ]
    }
   ],
   "source": [
    "# Divide infractions by # of sentences if there are dups on ID / DISCIPLINARY_INFRACTION_DATE\n",
    "    # might indicate concurrent sentences\n",
    "count_dups = disc_infraction.groupby(['ID','DISCIPLINARY_INFRACTION_DATE'])[\"ID\"].count().reset_index(name=\"count\")\n",
    "disc_infraction = disc_infraction.merge(count_dups, how = 'left')\n",
    "disc_infraction['INFRACTION_PER_SENT'] = round(disc_infraction['INFRACTION_PER_SENT']/disc_infraction['count'])\n",
    "\n",
    "disc_infraction = disc_infraction.loc[:,['ID','COMMITMENT_PREFIX','INFRACTION_PER_SENT']]\n",
    "print(\"Disc Infractions\",disc_infraction.shape)\n",
    "\n",
    "# Merge on disciplinary infractions, replace missing to 0\n",
    "dataset_flag = dataset_flag.merge(disc_infraction, how='left', on=['ID','COMMITMENT_PREFIX'])\n",
    "dataset_flag.loc[dataset_flag['INFRACTION_PER_SENT'].isnull(),'INFRACTION_PER_SENT'] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part E - calculating additional features - getting total # of felony charges and total # of misd charges from\n",
    "# sentence component\n",
    "sent_count_fel_misd = sent_comp_small.groupby(['ID','COMMITMENT_PREFIX','PRIMARY_FELONYMISDEMEANOR_CD']).size().reset_index(name='Count')\n",
    "sent_count_fel_misd = sent_count_fel_misd.set_index(['ID','COMMITMENT_PREFIX','PRIMARY_FELONYMISDEMEANOR_CD']).unstack().reset_index()\n",
    "sent_count_fel_misd.fillna(0, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>COMMITMENT_PREFIX</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRIMARY_FELONYMISDEMEANOR_CD</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>FELON</th>\n",
       "      <th>MISD.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000004</td>\n",
       "      <td>AA</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000006</td>\n",
       "      <td>AA</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000006</td>\n",
       "      <td>AB</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0000008</td>\n",
       "      <td>AA</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000008</td>\n",
       "      <td>AB</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   ID COMMITMENT_PREFIX Count      \n",
       "PRIMARY_FELONYMISDEMEANOR_CD                            FELON MISD.\n",
       "0                             0000004                AA   2.0   0.0\n",
       "1                             0000006                AA   0.0   1.0\n",
       "2                             0000006                AB   0.0  27.0\n",
       "3                             0000008                AA   0.0   1.0\n",
       "4                             0000008                AB   1.0   0.0"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_count_fel_misd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daminisharma/miniconda3/lib/python3.7/site-packages/pandas/core/reshape/merge.py:618: UserWarning: merging between different levels can give an unintended result (1 levels on the left, 2 on the right)\n",
      "  warnings.warn(msg, UserWarning)\n",
      "/Users/daminisharma/miniconda3/lib/python3.7/site-packages/pandas/core/generic.py:3936: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  obj = obj._drop_axis(labels, axis, level=level, errors=errors)\n"
     ]
    }
   ],
   "source": [
    "dataset_flag = dataset_flag.merge(sent_count_fel_misd, how='left', on =['ID','COMMITMENT_PREFIX'], right_index=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sent_count_fel_misd.set_index(['ID','COMMITMENT_PREFIX'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "#small = sent_count_fel_misd.sample(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_flag.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "#small.reset_index(level='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "#small = small.loc[:,['ID','COMMITMENT_PREFIX','count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "#small.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "#small.reset_index().drop('PRIMARY_FELONYMISDEMEANOR_CD',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sent_count_fel_misd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "#index = pd.MultiIndex(sent_count_fel_misd, names=['ID', 'COMMITMENT_PREFIX'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sent_count_fel_misd2 = sent_count_fel_misd.pivot(index=index,columns='PRIMARY_FELONYMISDEMEANOR_CD',values='count')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                                     0\n",
       "COMMITMENT_PREFIX                      0\n",
       "EARLIEST_SENTENCE_EFFECTIVE_DT         0\n",
       "MOST_SERIOUS_OFFENSE_CODE              0\n",
       "INMATE_COMPUTATION_STATUS_FLAG         0\n",
       "END_DATE                               0\n",
       "PROJ_END_DATE                          0\n",
       "INMATE_RECORD_STATUS_CODE           5930\n",
       "INMATE_ADMIN_STATUS_CODE            5930\n",
       "DATE_OF_LAST_INMATE_MOVEMENT        5930\n",
       "TYPE_OF_LAST_INMATE_MOVEMENT       20003\n",
       "CURRENT_COMMITMENT_PREFIX         190326\n",
       "CONTROL_STATUS                      5930\n",
       "GENDER                                 0\n",
       "RACE                                   2\n",
       "BIRTH_DATE                             0\n",
       "STATE_BORN                         30883\n",
       "ETHNICITY                            704\n",
       "CITIZENSHIP                          681\n",
       "PRIMARY_OFFENSE_CODE               44127\n",
       "NextPrefix                             0\n",
       "NextStart                         461384\n",
       "NextOffense                            0\n",
       "new_col                                0\n",
       "Time_Diff                         461384\n",
       "Recidivate                         37086\n",
       "INFRACTION_PER_SENT                    0\n",
       "(Count, FELON)                        47\n",
       "(Count, MISD.)                        47\n",
       "dtype: int64"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataset_flag = dataset_flag.merge(sent_count_fel_misd, how='left', on=['ID','COMMITMENT_PREFIX'])\n",
    "dataset_flag.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hold out active sentences\n",
      "Size of active sentences dataset:  32801\n",
      "Drop observations with no recidivism flag (this will also drop active sentences, but we've already separated those)\n",
      "Additional observations dropped are mostly of those who died in prison and therefore wont have a recidivate flag\n",
      "Size of remaining dataset:  827731\n",
      "Merging on our coded categories\n",
      "% missing decided category 0.007752518632260964\n",
      "Dataset size:  821314\n"
     ]
    }
   ],
   "source": [
    "# Part F\n",
    "# Step 5\n",
    "# Hold out active senteces\n",
    "print(\"Hold out active sentences\")\n",
    "active_sentences = dataset_flag[(dataset_flag['INMATE_ADMIN_STATUS_CODE']=='ACTIVE') & (dataset_flag['NextPrefix']==0)]\n",
    "print(\"Size of active sentences dataset: \",active_sentences.shape[0])\n",
    "\n",
    "# Step 6\n",
    "# drop observations with no recidivism flag (this will also drop active sentences, but we've already separated those)\n",
    "print(\"Drop observations with no recidivism flag (this will also drop active sentences, but we've already separated those)\")\n",
    "print(\"Additional observations dropped are mostly of those who died in prison and therefore wont have a recidivate flag\")\n",
    "dataset_flag = dataset_flag[(dataset_flag['Recidivate'].notnull())]\n",
    "print(\"Size of remaining dataset: \",dataset_flag.shape[0])\n",
    "\n",
    "# Step 7\n",
    "# Bring in coded offenses - sanity check\n",
    "\n",
    "# this merges our coded offenses onto \"most serious offense\" to check how much coverage\n",
    "# our variable is giving us. however, this not what we ultimately want - in the end, we want\n",
    "# our codes to be merged onto \"nextOffense\" - i.e., the offense code for the next offense \n",
    "# someone committed that resulted in re-incarceration\n",
    "# NextOffense can be missing for 2 reasons: because most serious offense is missing, or because\n",
    "# the individual did not recidivate. after merging our codes onto \"NextOffense\", we can replace\n",
    "# \"Decided Category\" with 0 if recidivism = 0, and leave it as NA otherwise\n",
    "#dataset_with_offenses_test = dataset_flag.merge(coded_offenses, how='left', left_on='MOST_SERIOUS_OFFENSE_CODE', right_on='Primary offense code')\n",
    "\n",
    "# Step 8 and 9\n",
    "# Now, merge on coded offenses onto NextOffense, turn Decided Category, More Lenient, and more harsh = 0 if recidivism = 0\n",
    "print(\"Merging on our coded categories\")\n",
    "dataset_with_offenses = dataset_flag.merge(coded_offenses, how='left', left_on='NextOffense', right_on='Primary offense code')\n",
    "dataset_with_offenses.loc[dataset_with_offenses['Recidivate']==0,'Decided Category'] = 0\n",
    "dataset_with_offenses.loc[dataset_with_offenses['Recidivate']==0,'More lenient'] = 0\n",
    "dataset_with_offenses.loc[dataset_with_offenses['Recidivate']==0,'More harsh'] = 0\n",
    "\n",
    "print(\"% missing decided category\",dataset_with_offenses['Decided Category'].isnull().sum()/dataset_with_offenses.shape[0])\n",
    "\n",
    "# Drop those missing decided category\n",
    "dataset_with_offenses = dataset_with_offenses[(dataset_with_offenses['Decided Category'].notnull())]\n",
    "print(\"Dataset size: \" , dataset_with_offenses.shape[0])\n",
    "\n",
    "\n",
    "# Step 10 \n",
    "# Add active sentences back in so we can merge our coded categories onto Most Serious Offense and so \n",
    "# all the data is together when we construct features we'll need before pre processing (e.g. economic vars,\n",
    "# age vars)\n",
    "dataset_with_offenses = dataset_with_offenses.append(active_sentences)\n",
    "# Rename Columns\n",
    "dataset_with_offenses = dataset_with_offenses.rename(columns ={'Decided Category':'Recidivate_Risk_Level'})\n",
    "dataset_with_offenses = dataset_with_offenses.rename(columns ={'More lenient':'Recidivate_Risk_Level_Lenient'})\n",
    "dataset_with_offenses = dataset_with_offenses.rename(columns ={'More harsh':'Recidivate_Risk_Level_Harsh'})\n",
    "\n",
    "dataset_with_offenses = dataset_with_offenses.merge(coded_offenses, how='left', left_on='MOST_SERIOUS_OFFENSE_CODE', right_on='Primary offense code')\n",
    "dataset_with_offenses = dataset_with_offenses.rename(columns = {'Decided Category':'Current_Offense_Risk_Level','More lenient':'Current_Offense_Risk_Level_Lenient','More harsh':'Current_Offense_Risk_Level_Harsh'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_with_offenses = dataset_with_offenses.append(active_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>new_col</th>\n",
       "      <th>Time_Diff</th>\n",
       "      <th>Recidivate</th>\n",
       "      <th>INFRACTION_PER_SENT</th>\n",
       "      <th>(Count, FELON)</th>\n",
       "      <th>(Count, MISD.)</th>\n",
       "      <th>Recidivate_Risk_Level</th>\n",
       "      <th>Recidivate_Risk_Level_Lenient</th>\n",
       "      <th>Recidivate_Risk_Level_Harsh</th>\n",
       "      <th>Current_Offense_Risk_Level</th>\n",
       "      <th>Current_Offense_Risk_Level_Lenient</th>\n",
       "      <th>Current_Offense_Risk_Level_Harsh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>854115.000000</td>\n",
       "      <td>397016.000000</td>\n",
       "      <td>821314.000000</td>\n",
       "      <td>854115.000000</td>\n",
       "      <td>854072.000000</td>\n",
       "      <td>854072.000000</td>\n",
       "      <td>821314.000000</td>\n",
       "      <td>821314.000000</td>\n",
       "      <td>821314.000000</td>\n",
       "      <td>807864.000000</td>\n",
       "      <td>807864.000000</td>\n",
       "      <td>807864.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>930.329291</td>\n",
       "      <td>3.624128</td>\n",
       "      <td>0.171411</td>\n",
       "      <td>3.898150</td>\n",
       "      <td>1.058833</td>\n",
       "      <td>0.843916</td>\n",
       "      <td>0.390327</td>\n",
       "      <td>0.323152</td>\n",
       "      <td>0.467961</td>\n",
       "      <td>2.351235</td>\n",
       "      <td>1.994487</td>\n",
       "      <td>2.758052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>998.275713</td>\n",
       "      <td>4.158104</td>\n",
       "      <td>0.376868</td>\n",
       "      <td>14.386806</td>\n",
       "      <td>1.242654</td>\n",
       "      <td>1.528526</td>\n",
       "      <td>0.961667</td>\n",
       "      <td>0.814176</td>\n",
       "      <td>1.145918</td>\n",
       "      <td>1.087621</td>\n",
       "      <td>1.002935</td>\n",
       "      <td>1.225553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-265.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2002.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2020.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1009.000000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>125.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             new_col      Time_Diff     Recidivate  INFRACTION_PER_SENT  \\\n",
       "count  854115.000000  397016.000000  821314.000000        854115.000000   \n",
       "mean      930.329291       3.624128       0.171411             3.898150   \n",
       "std       998.275713       4.158104       0.376868            14.386806   \n",
       "min         0.000000    -265.000000       0.000000             0.000000   \n",
       "25%         0.000000       1.000000       0.000000             0.000000   \n",
       "50%         0.000000       2.000000       0.000000             0.000000   \n",
       "75%      2002.000000       5.000000       0.000000             2.000000   \n",
       "max      2020.000000      45.000000       1.000000          1009.000000   \n",
       "\n",
       "       (Count, FELON)  (Count, MISD.)  Recidivate_Risk_Level  \\\n",
       "count   854072.000000   854072.000000          821314.000000   \n",
       "mean         1.058833        0.843916               0.390327   \n",
       "std          1.242654        1.528526               0.961667   \n",
       "min          0.000000        0.000000               0.000000   \n",
       "25%          0.000000        0.000000               0.000000   \n",
       "50%          1.000000        0.000000               0.000000   \n",
       "75%          1.000000        1.000000               0.000000   \n",
       "max         87.000000      125.000000               5.000000   \n",
       "\n",
       "       Recidivate_Risk_Level_Lenient  Recidivate_Risk_Level_Harsh  \\\n",
       "count                  821314.000000                821314.000000   \n",
       "mean                        0.323152                     0.467961   \n",
       "std                         0.814176                     1.145918   \n",
       "min                         0.000000                     0.000000   \n",
       "25%                         0.000000                     0.000000   \n",
       "50%                         0.000000                     0.000000   \n",
       "75%                         0.000000                     0.000000   \n",
       "max                         5.000000                     5.000000   \n",
       "\n",
       "       Current_Offense_Risk_Level  Current_Offense_Risk_Level_Lenient  \\\n",
       "count               807864.000000                       807864.000000   \n",
       "mean                     2.351235                            1.994487   \n",
       "std                      1.087621                            1.002935   \n",
       "min                      1.000000                            1.000000   \n",
       "25%                      1.000000                            1.000000   \n",
       "50%                      2.000000                            2.000000   \n",
       "75%                      3.000000                            2.000000   \n",
       "max                      5.000000                            5.000000   \n",
       "\n",
       "       Current_Offense_Risk_Level_Harsh  \n",
       "count                     807864.000000  \n",
       "mean                           2.758052  \n",
       "std                            1.225553  \n",
       "min                            1.000000  \n",
       "25%                            2.000000  \n",
       "50%                            3.000000  \n",
       "75%                            4.000000  \n",
       "max                            5.000000  "
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_with_offenses.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_with_offenses.to_csv('../data/dataset_main_active.csv', index=False)\n",
    "#active_sentences.to_csv('../data/active_sentences.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>COMMITMENT_PREFIX</th>\n",
       "      <th>EARLIEST_SENTENCE_EFFECTIVE_DT</th>\n",
       "      <th>MOST_SERIOUS_OFFENSE_CODE</th>\n",
       "      <th>INMATE_COMPUTATION_STATUS_FLAG</th>\n",
       "      <th>END_DATE</th>\n",
       "      <th>PROJ_END_DATE</th>\n",
       "      <th>INMATE_RECORD_STATUS_CODE</th>\n",
       "      <th>INMATE_ADMIN_STATUS_CODE</th>\n",
       "      <th>DATE_OF_LAST_INMATE_MOVEMENT</th>\n",
       "      <th>TYPE_OF_LAST_INMATE_MOVEMENT</th>\n",
       "      <th>CURRENT_COMMITMENT_PREFIX</th>\n",
       "      <th>CONTROL_STATUS</th>\n",
       "      <th>GENDER</th>\n",
       "      <th>RACE</th>\n",
       "      <th>BIRTH_DATE</th>\n",
       "      <th>STATE_BORN</th>\n",
       "      <th>ETHNICITY</th>\n",
       "      <th>CITIZENSHIP</th>\n",
       "      <th>PRIMARY_OFFENSE_CODE</th>\n",
       "      <th>NextPrefix</th>\n",
       "      <th>NextStart</th>\n",
       "      <th>NextOffense</th>\n",
       "      <th>new_col</th>\n",
       "      <th>Time_Diff</th>\n",
       "      <th>Recidivate</th>\n",
       "      <th>INFRACTION_PER_SENT</th>\n",
       "      <th>(Count, FELON)</th>\n",
       "      <th>(Count, MISD.)</th>\n",
       "      <th>Primary offense code_x</th>\n",
       "      <th>Description (if needed)_x</th>\n",
       "      <th>Recidivate_Risk_Level</th>\n",
       "      <th>Needed a check?_x</th>\n",
       "      <th>Recidivate_Risk_Level_Lenient</th>\n",
       "      <th>Recidivate_Risk_Level_Harsh</th>\n",
       "      <th>Primary offense code_y</th>\n",
       "      <th>Description (if needed)_y</th>\n",
       "      <th>Current_Offense_Risk_Level</th>\n",
       "      <th>Needed a check?_y</th>\n",
       "      <th>Current_Offense_Risk_Level_Lenient</th>\n",
       "      <th>Current_Offense_Risk_Level_Harsh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000004</td>\n",
       "      <td>AA</td>\n",
       "      <td>1983-07-12</td>\n",
       "      <td>SELL SCHEDULE II</td>\n",
       "      <td>EXPIRED</td>\n",
       "      <td>1984-07-11</td>\n",
       "      <td>1984-07-11</td>\n",
       "      <td>INACTIVE</td>\n",
       "      <td>INACTIVE</td>\n",
       "      <td>1984-07-11</td>\n",
       "      <td>TERMINATED PAROLE</td>\n",
       "      <td>None</td>\n",
       "      <td>REGULAR POPULATION        RPOP</td>\n",
       "      <td>MALE</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>1961-10-15</td>\n",
       "      <td>ALASKA</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>BORN IN U.S.</td>\n",
       "      <td>SELL SCHEDULE II</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>SELL SCHEDULE II</td>\n",
       "      <td>Selling drugs on Schedule II - high potential ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>YES</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000006</td>\n",
       "      <td>AA</td>\n",
       "      <td>1973-01-30</td>\n",
       "      <td>WORTHLESS CHECK</td>\n",
       "      <td>EXPIRED</td>\n",
       "      <td>1973-03-28</td>\n",
       "      <td>0001-01-01</td>\n",
       "      <td>INACTIVE</td>\n",
       "      <td>INACTIVE</td>\n",
       "      <td>1975-08-18</td>\n",
       "      <td>TERMINATED PAROLE</td>\n",
       "      <td>None</td>\n",
       "      <td>REGULAR POPULATION        RPOP</td>\n",
       "      <td>MALE</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>1951-07-17</td>\n",
       "      <td>NORTH CAROLINA</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>BORN IN U.S.</td>\n",
       "      <td>WORTHLESS CHECK</td>\n",
       "      <td>AB</td>\n",
       "      <td>1973-04-11</td>\n",
       "      <td>WORTHLESS CHECK</td>\n",
       "      <td>1973</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>WORTHLESS CHECK</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NO</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>WORTHLESS CHECK</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NO</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000006</td>\n",
       "      <td>AB</td>\n",
       "      <td>1973-04-11</td>\n",
       "      <td>WORTHLESS CHECK</td>\n",
       "      <td>EXPIRED</td>\n",
       "      <td>1975-08-18</td>\n",
       "      <td>1974-08-10</td>\n",
       "      <td>INACTIVE</td>\n",
       "      <td>INACTIVE</td>\n",
       "      <td>1975-08-18</td>\n",
       "      <td>TERMINATED PAROLE</td>\n",
       "      <td>None</td>\n",
       "      <td>REGULAR POPULATION        RPOP</td>\n",
       "      <td>MALE</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>1951-07-17</td>\n",
       "      <td>NORTH CAROLINA</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>BORN IN U.S.</td>\n",
       "      <td>WORTHLESS CHECK</td>\n",
       "      <td>0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>WORTHLESS CHECK</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NO</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0000008</td>\n",
       "      <td>AA</td>\n",
       "      <td>1990-04-09</td>\n",
       "      <td>DWI DRIVING WHILE IMPAIRED</td>\n",
       "      <td>EXPIRED</td>\n",
       "      <td>1990-05-17</td>\n",
       "      <td>1990-10-09</td>\n",
       "      <td>INACTIVE</td>\n",
       "      <td>INACTIVE</td>\n",
       "      <td>1995-09-14</td>\n",
       "      <td>TERMINATED PAROLE</td>\n",
       "      <td>None</td>\n",
       "      <td>REGULAR POPULATION        RPOP</td>\n",
       "      <td>MALE</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>1963-12-29</td>\n",
       "      <td>NORTH CAROLINA</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>BORN IN U.S.</td>\n",
       "      <td>DWI DRIVING WHILE IMPAIRED</td>\n",
       "      <td>AB</td>\n",
       "      <td>1993-08-30</td>\n",
       "      <td>HABITUAL IMPAIRED DRIVING</td>\n",
       "      <td>1993</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>HABITUAL IMPAIRED DRIVING</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NO</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>DWI DRIVING WHILE IMPAIRED</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NO</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000008</td>\n",
       "      <td>AB</td>\n",
       "      <td>1993-08-30</td>\n",
       "      <td>HABITUAL IMPAIRED DRIVING</td>\n",
       "      <td>EXPIRED</td>\n",
       "      <td>1994-01-26</td>\n",
       "      <td>1994-02-18</td>\n",
       "      <td>INACTIVE</td>\n",
       "      <td>INACTIVE</td>\n",
       "      <td>1995-09-14</td>\n",
       "      <td>TERMINATED PAROLE</td>\n",
       "      <td>None</td>\n",
       "      <td>REGULAR POPULATION        RPOP</td>\n",
       "      <td>MALE</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>1963-12-29</td>\n",
       "      <td>NORTH CAROLINA</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>BORN IN U.S.</td>\n",
       "      <td>HABITUAL IMPAIRED DRIVING</td>\n",
       "      <td>BA</td>\n",
       "      <td>1995-01-02</td>\n",
       "      <td>HABITUAL IMPAIRED DRIVING</td>\n",
       "      <td>1995</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>HABITUAL IMPAIRED DRIVING</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NO</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>HABITUAL IMPAIRED DRIVING</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NO</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID COMMITMENT_PREFIX EARLIEST_SENTENCE_EFFECTIVE_DT  \\\n",
       "0  0000004                AA                     1983-07-12   \n",
       "1  0000006                AA                     1973-01-30   \n",
       "2  0000006                AB                     1973-04-11   \n",
       "3  0000008                AA                     1990-04-09   \n",
       "4  0000008                AB                     1993-08-30   \n",
       "\n",
       "    MOST_SERIOUS_OFFENSE_CODE INMATE_COMPUTATION_STATUS_FLAG   END_DATE  \\\n",
       "0            SELL SCHEDULE II                        EXPIRED 1984-07-11   \n",
       "1             WORTHLESS CHECK                        EXPIRED 1973-03-28   \n",
       "2             WORTHLESS CHECK                        EXPIRED 1975-08-18   \n",
       "3  DWI DRIVING WHILE IMPAIRED                        EXPIRED 1990-05-17   \n",
       "4   HABITUAL IMPAIRED DRIVING                        EXPIRED 1994-01-26   \n",
       "\n",
       "  PROJ_END_DATE INMATE_RECORD_STATUS_CODE INMATE_ADMIN_STATUS_CODE  \\\n",
       "0    1984-07-11                  INACTIVE                 INACTIVE   \n",
       "1    0001-01-01                  INACTIVE                 INACTIVE   \n",
       "2    1974-08-10                  INACTIVE                 INACTIVE   \n",
       "3    1990-10-09                  INACTIVE                 INACTIVE   \n",
       "4    1994-02-18                  INACTIVE                 INACTIVE   \n",
       "\n",
       "  DATE_OF_LAST_INMATE_MOVEMENT TYPE_OF_LAST_INMATE_MOVEMENT  \\\n",
       "0                   1984-07-11            TERMINATED PAROLE   \n",
       "1                   1975-08-18            TERMINATED PAROLE   \n",
       "2                   1975-08-18            TERMINATED PAROLE   \n",
       "3                   1995-09-14            TERMINATED PAROLE   \n",
       "4                   1995-09-14            TERMINATED PAROLE   \n",
       "\n",
       "  CURRENT_COMMITMENT_PREFIX                  CONTROL_STATUS GENDER   RACE  \\\n",
       "0                      None  REGULAR POPULATION        RPOP   MALE  WHITE   \n",
       "1                      None  REGULAR POPULATION        RPOP   MALE  WHITE   \n",
       "2                      None  REGULAR POPULATION        RPOP   MALE  WHITE   \n",
       "3                      None  REGULAR POPULATION        RPOP   MALE  WHITE   \n",
       "4                      None  REGULAR POPULATION        RPOP   MALE  WHITE   \n",
       "\n",
       "   BIRTH_DATE      STATE_BORN ETHNICITY   CITIZENSHIP  \\\n",
       "0  1961-10-15          ALASKA   UNKNOWN  BORN IN U.S.   \n",
       "1  1951-07-17  NORTH CAROLINA   UNKNOWN  BORN IN U.S.   \n",
       "2  1951-07-17  NORTH CAROLINA   UNKNOWN  BORN IN U.S.   \n",
       "3  1963-12-29  NORTH CAROLINA   UNKNOWN  BORN IN U.S.   \n",
       "4  1963-12-29  NORTH CAROLINA   UNKNOWN  BORN IN U.S.   \n",
       "\n",
       "         PRIMARY_OFFENSE_CODE NextPrefix  NextStart  \\\n",
       "0            SELL SCHEDULE II          0        NaT   \n",
       "1             WORTHLESS CHECK         AB 1973-04-11   \n",
       "2             WORTHLESS CHECK          0        NaT   \n",
       "3  DWI DRIVING WHILE IMPAIRED         AB 1993-08-30   \n",
       "4   HABITUAL IMPAIRED DRIVING         BA 1995-01-02   \n",
       "\n",
       "                 NextOffense  new_col  Time_Diff  Recidivate  \\\n",
       "0                          0        0        NaN         0.0   \n",
       "1            WORTHLESS CHECK     1973        0.0         1.0   \n",
       "2                          0        0        NaN         0.0   \n",
       "3  HABITUAL IMPAIRED DRIVING     1993        3.0         0.0   \n",
       "4  HABITUAL IMPAIRED DRIVING     1995        1.0         1.0   \n",
       "\n",
       "   INFRACTION_PER_SENT  (Count, FELON)  (Count, MISD.)  \\\n",
       "0                  0.0             2.0             0.0   \n",
       "1                  0.0             0.0             1.0   \n",
       "2                  0.0             0.0            27.0   \n",
       "3                  0.0             0.0             1.0   \n",
       "4                  0.0             1.0             0.0   \n",
       "\n",
       "      Primary offense code_x Description (if needed)_x  Recidivate_Risk_Level  \\\n",
       "0                        NaN                       NaN                    0.0   \n",
       "1            WORTHLESS CHECK                         0                    1.0   \n",
       "2                        NaN                       NaN                    0.0   \n",
       "3  HABITUAL IMPAIRED DRIVING                         0                    0.0   \n",
       "4  HABITUAL IMPAIRED DRIVING                         0                    3.0   \n",
       "\n",
       "  Needed a check?_x  Recidivate_Risk_Level_Lenient  \\\n",
       "0               NaN                            0.0   \n",
       "1                NO                            1.0   \n",
       "2               NaN                            0.0   \n",
       "3                NO                            0.0   \n",
       "4                NO                            3.0   \n",
       "\n",
       "   Recidivate_Risk_Level_Harsh      Primary offense code_y  \\\n",
       "0                          0.0            SELL SCHEDULE II   \n",
       "1                          1.0             WORTHLESS CHECK   \n",
       "2                          0.0             WORTHLESS CHECK   \n",
       "3                          0.0  DWI DRIVING WHILE IMPAIRED   \n",
       "4                          3.0   HABITUAL IMPAIRED DRIVING   \n",
       "\n",
       "                           Description (if needed)_y  \\\n",
       "0  Selling drugs on Schedule II - high potential ...   \n",
       "1                                                  0   \n",
       "2                                                  0   \n",
       "3                                                  0   \n",
       "4                                                  0   \n",
       "\n",
       "   Current_Offense_Risk_Level Needed a check?_y  \\\n",
       "0                         3.0               YES   \n",
       "1                         1.0                NO   \n",
       "2                         1.0                NO   \n",
       "3                         2.0                NO   \n",
       "4                         3.0                NO   \n",
       "\n",
       "   Current_Offense_Risk_Level_Lenient  Current_Offense_Risk_Level_Harsh  \n",
       "0                                 2.0                               4.0  \n",
       "1                                 1.0                               1.0  \n",
       "2                                 1.0                               1.0  \n",
       "3                                 2.0                               2.0  \n",
       "4                                 3.0                               3.0  "
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_with_offenses.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "#active_sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count = active_sentences.groupby(\"MOST_SERIOUS_OFFENSE_CODE\").size().reset_index(name=\"count\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count = active_sentences.groupby('MOST_SERIOUS_OFFENSE_CODE')['ID'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count.sort_values(by='count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As detailed above, here's where we stand with \"most serious offense code\"\n",
    "    - 33% of dataset A is missing most_serious_offense\n",
    "    - using sentence component, we created primary offense code for about 92% of the sentence component data (using minimum and maximum length) \n",
    "    - this variable (call it Offense_Constructed) has a 93% match rate with MosT Serious Offense in dataset A (where its available)\n",
    "    - we're going to use Most Serious Offense where available (66% of the time), replace with Offense_Constructed where Most Serious Offense is unavailable and Offense_Constructed is available (32% of data). \n",
    "    - This will mean we are still missing Most Serious Offense for 4% of observations. Not all of these will be relevant to our outcome variable (only relevant when someone recidivates) but a) we want to use most serious offense as a predictor so missingness is relevant and b) how many of these are relevant for recidivating might keep changing depending on our # of years for recidivating. After we do all other changes to this dataset (e.g. dropping for weird dates) will check again how many are missing most serious offense. will drop those at that point (2.6% obs)\n",
    "    - Finally, ~1% of the remaining data is missing our outcome variable once it is merged on because we only coded\n",
    "       up offenses that took up 95% of all offenses. We drop these as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part C broken down in more detail\n",
    "    # 1. Deal with date issues (takes us from 903,000 obs to 888,120). For more details, see below:\n",
    "        # a. replace end date with projected end date where END_DATE = 0001-01-01 (placeholder for missing)\n",
    "        # b. drop observations still missing end_date (should be only about ~350 observations)\n",
    "        # c. drop observations missing EARLIEST_SENTENCE_EFFECTIVE_DT (about 12k observations)\n",
    "        # b and c are dropping those where the sentence is either only in court commitment or only in sentence comp\n",
    "    # 2. Query the remaining dataset to get the the next commitment prefix, next sentence date, and most serious\n",
    "        # offense code for the next observation - where all of these exist. for a sentence that does not result in \n",
    "        # recidivism, nextprefix, nextstart, nextoffense will be 0\n",
    "    # 3. Clean up dates - turn them into date format, after recoding the top coded 9999 dates (for life sentences)\n",
    "    # 4. Get recidivism flags. See decision rule below\n",
    "    # 5. Hold out active sentences (~approx 32,000 obs)\n",
    "    # 6. Drop observation with no recidivism flag (Takes us from 888,120 to 850970, i.e. dropping\n",
    "        # 38,000 observations. 32k of those are active sentences, 6k are \"out of universe\" i.e.\n",
    "        # sentences that are expired but the individual was never released (mostly death in prison)\n",
    "    # 7. Sanity check - Merge on our coded offenses to most serious offenses and see how well we cover the offenses\n",
    "        # Approx 5% of observations that have Most Serious Offense do NOT have \"Decided Category\" (our variable)\n",
    "        # This makes sense because we only coded up offenses that made up 95% of the offenses\n",
    "    # 8. Merge on our coded offenses to \"NextOffense\" - the relevant variable now is \"Decided category\".\n",
    "    # 9. Replace Decided Category to 0 if recidivism = 0 ; leave it as NA otherwise\n",
    "        # After holding out active sentences and dropping \"out of universe observations\", we have ~850k observations\n",
    "        # of these, we are missing a \"Decided Category\" flag (as defined by our coded offenses) for 7% of the data\n",
    "        # this is a lot better than missing it for 33% of the data (since we're missing \"most serious offense\" for \n",
    "        # 33% of the data) but its still not great - hopefully once we bring in most serious offense from sentence\n",
    "        # component, we can reduce 7% down to something more negligible\n",
    "    # 10. Understand the missingness of our possible features\n",
    "    \n",
    "# We now have two datasets that are ready for pre processing and feature engineering:\n",
    "    # dataset_with_offenses = datasetA \n",
    "    # active_sentences = data on which we will apply our predictions\n",
    "        # Next steps (I think): develop a list of features and functions that can clean up those features, which can \n",
    "        # be applied to both of the datasets above\n",
    "        # Additionally - do we want to write both of these to csv that we push to github?\n",
    "    \n",
    "# More details on Dates\n",
    "    # In addition to the dates that are null (see above) because some data exists in court commitment\n",
    "    # that doesnt exist in sentence computation (and vice versa) we also have start and end\n",
    "    # dates that are 0001-01-01 - based on looking up some offenders with these dates, these\n",
    "    # are often just missing so 0001-01-01 is a placeholder for missing date\n",
    "\n",
    "    # There are about 10k observations with end_date = 0001-01-01. These don't seem random -\n",
    "    # 9k of these are for the commitment prefix BA, and on spot checking many of them look like\n",
    "    # the sentences were categorized as \"FAIR FELONS\" - related to the fair sentencing act that\n",
    "    # affects sentences from 1982 to October 1994 (before NC enacted structured sentencing which\n",
    "    # abolished parole). It also seems like many of those sentences are missing an \"actual release\n",
    "    # date\" from prison but have a release date from parole\n",
    "    # \n",
    "    # Where available, the end date will be replaced with the projected release date. on spot\n",
    "    # checking, this seems to be a reasonable proxy for when inmate was last moved\n",
    "    # There are 397 observations missing both end date and projected end date - dropping these\n",
    "    #\n",
    "    # About 12k observations have start date = 0001-01-01. On spot checking, some of these\n",
    "    # appear to be entirely missing from sentence component and from the offender's online\n",
    "    # profile - as if the sentences were removed ex-post. Since there is no way to get a start\n",
    "    # date for these, they will be dropped. Approx 1% of the data\n",
    "\n",
    "# Note on \"Active\" Flag    \n",
    "    # To get \"Active\" sentences, we should probably not trust the Inmate Commitment\n",
    "    # status flag in court commitment. This often appears active even for sentences that\n",
    "    # online show \"service status\" = \"Expired\"\n",
    "\n",
    "    # instead, we should merge on information from INmate Profile. This has \"inmate record status\"\n",
    "    # and \"inmate admin status\". After some exploration, it seems like admin status = active\n",
    "    # means one is in prison; record status = active (if admin status = inactive) is mostly for\n",
    "    # people on parole/probation.\n",
    "\n",
    "\n",
    "# Decision rule for recidivism flag:\n",
    "    # if NextPrefix != 0: if nextStart - endDate is less than XXX (make this a parameter) then recidivism = 1 else 0\n",
    "\n",
    "    # if nextprefix = 0, inmate is inactive, and they did not die in prison \n",
    "    # (e.g. serving life sentence or  other wise) then \n",
    "    # recidivism = 0\n",
    "\n",
    "    # if nextprefix = 0, inmate status code is not active or inactive (could be missing) and \n",
    "    # end date is not 2261-01-02 (life sentence), they were likely released from prison\n",
    "    # recidivism = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
